{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import spacy, random, math, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.datasets import TranslationDataset, Multi30k, IWSLT\n",
    "from torchtext.data import Field, BucketIterator, RawField, Dataset\n",
    "\n",
    "from models.gcn import GCNLayer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi30k\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 11747\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text, reverse=False):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    tokens = [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "    return tokens[::-1] if reverse else tokens\n",
    "    \n",
    "def tokenize_en(text, reverse=False):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    tokens = [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "    return tokens[::-1] if reverse else tokens\n",
    "    \n",
    "def batch_graph(grhs):\n",
    "    \"\"\" batch a list of graphs\n",
    "    @param grhs: list(tensor,...) \n",
    "    \"\"\"\n",
    "    b = len(grhs)  # batch size\n",
    "    graph_dims = [len(g) for g in grhs]\n",
    "    s = max(graph_dims)  # max seq length\n",
    "    \n",
    "    G = torch.zeros([b, s, s])\n",
    "    for i, g in enumerate(grhs):\n",
    "        s_ = graph_dims[i]\n",
    "        G[i,:s_,:s_] = g\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVERSE = True\n",
    "SRC = Field(tokenize = lambda text: tokenize_de(text, REVERSE), \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "TGT = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "GRH = RawField(postprocessing=batch_graph)\n",
    "data_fields = [('src', SRC), ('trg', TGT), ('grh', GRH)]\n",
    "\n",
    "train_data = Dataset(torch.load(\"data/Multi30k/train_data.pt\"), data_fields)\n",
    "valid_data = Dataset(torch.load(\"data/Multi30k/valid_data.pt\"), data_fields)\n",
    "test_data = Dataset(torch.load(\"data/Multi30k/test_data.pt\"), data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    sort_key = lambda x: len(x.src),\n",
    "    sort_within_batch=False,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n",
      "Unique tokens in source (de) vocabulary: 7855\n",
      "Unique tokens in target (en) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TGT.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TGT.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_sentence_lengths(dataset):\n",
    "    src_counter = Counter()\n",
    "    tgt_counter = Counter()\n",
    "    for exp in dataset:\n",
    "        src_counter[len(exp.src)] += 1\n",
    "        tgt_counter[len(exp.trg)] += 1\n",
    "    return src_counter, tgt_counter\n",
    "\n",
    "def counter2array(counter):\n",
    "    result = []\n",
    "    for k in counter:\n",
    "        result.extend([k for _ in range(counter[k])])\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum src, tgt sent lengths: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(44, 41)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_c, tgt_c = get_sentence_lengths(train_data)\n",
    "src_lengths = counter2array(src_c)\n",
    "tgt_lengths = counter2array(tgt_c)\n",
    "\n",
    "print(\"maximum src, tgt sent lengths: \")\n",
    "np.quantile(src_lengths, 1), np.quantile(tgt_lengths, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment with just GRU\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUEncoder(nn.Module):\n",
    "    def __init__(self, ninp, nembed, enc_nhid, dec_nhid, nlayers, dropout):\n",
    "        super(GRUEncoder, self).__init__()\n",
    "        self.enc_nhid = enc_nhid\n",
    "        self.dec_nhid = dec_nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.embedding = nn.Embedding(ninp, nembed)\n",
    "        self.rnn = nn.GRU(nembed, enc_nhid, nlayers, bidirectional=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(enc_nhid*2, dec_nhid)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src: (src len, b)\n",
    "        s, b = src.shape\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        out, hidden = self.rnn(embedded)\n",
    "        hidden = hidden.transpose(1,0).reshape(b, self.nlayers, -1).transpose(1,0)\n",
    "        hidden = torch.tanh(self.fc(hidden))\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUDecoder(nn.Module):\n",
    "    def __init__(self, nout, nembed, enc_nhid, dec_nhid, nlayers, dropout):\n",
    "        super(GRUDecoder, self).__init__()\n",
    "        self.nout = nout\n",
    "        self.enc_nhid = enc_nhid\n",
    "        self.dec_nhid = dec_nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.embedding = nn.Embedding(nout, nembed)\n",
    "        self.rnn = nn.GRU(nembed, dec_nhid, nlayers, bidirectional=False, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(dec_nhid, nout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = x.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        out, hidden = self.rnn(embedded, hidden)\n",
    "        pred = self.fc_out(out.squeeze(0))\n",
    "        return pred, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        # src: (src_len, b)\n",
    "        # tgt: (tgt_len, b)\n",
    "        tgt_len, b = tgt.shape\n",
    "        tgt_vocab_size = self.decoder.nout\n",
    "        \n",
    "        # tensor to store decoder outputs\n",
    "        outs = torch.zeros(tgt_len, b, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        enc_out, hidden = self.encoder(src)\n",
    "        x = tgt[0]\n",
    "        for t in range(1, tgt_len):\n",
    "            out, hidden = self.decoder(x, hidden)\n",
    "            outs[t] = out\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = out.argmax(1)\n",
    "            x = tgt[t] if teacher_force else top1\n",
    "            \n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TGT.vocab)\n",
    "ENC_EMB_DIM = 250\n",
    "DEC_EMB_DIM = 250\n",
    "ENC_HID_DIM = 500\n",
    "DEC_HID_DIM = 500\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "NLAYERS = 2\n",
    "\n",
    "enc = GRUEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, NLAYERS, ENC_DROPOUT)\n",
    "dec = GRUDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, NLAYERS, DEC_DROPOUT)\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): GRUEncoder(\n",
       "    (embedding): Embedding(7855, 250)\n",
       "    (rnn): GRU(250, 500, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "    (fc): Linear(in_features=1000, out_features=500, bias=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       "  (decoder): GRUDecoder(\n",
       "    (embedding): Embedding(5893, 250)\n",
       "    (rnn): GRU(250, 500, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=500, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 16,282,893 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "TGT_PAD_IDX = TGT.vocab.stoi[TGT.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TGT_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        tgt = batch.trg\n",
    "        optimizer.zero_grad()\n",
    "        out = model(src, tgt)\n",
    "        out_dim = out.shape[-1]\n",
    "        out = out[1:].view(-1, out_dim)\n",
    "        tgt = tgt[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(out, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss/len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            tgt = batch.trg\n",
    "            out = model(src, tgt, 0) # no teacher forcing here\n",
    "            out_dim = out.shape[-1]\n",
    "            out = out[1:].view(-1, out_dim)\n",
    "            tgt = tgt[1:].view(-1)\n",
    "            loss += criterion(out, tgt).item()\n",
    "    return loss/len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start, end):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 30s\n",
      "\tTrain Loss: 4.831 | Train PPL: 125.397\n",
      "\t Val. Loss: 4.563 |  Val. PPL:  95.885\n",
      "Epoch: 02 | Time: 0m 31s\n",
      "\tTrain Loss: 3.986 | Train PPL:  53.834\n",
      "\t Val. Loss: 4.439 |  Val. PPL:  84.731\n",
      "Epoch: 03 | Time: 0m 31s\n",
      "\tTrain Loss: 3.609 | Train PPL:  36.943\n",
      "\t Val. Loss: 4.183 |  Val. PPL:  65.545\n",
      "Epoch: 04 | Time: 0m 30s\n",
      "\tTrain Loss: 3.323 | Train PPL:  27.757\n",
      "\t Val. Loss: 4.023 |  Val. PPL:  55.895\n",
      "Epoch: 05 | Time: 0m 30s\n",
      "\tTrain Loss: 3.146 | Train PPL:  23.243\n",
      "\t Val. Loss: 3.973 |  Val. PPL:  53.149\n",
      "Epoch: 06 | Time: 0m 30s\n",
      "\tTrain Loss: 2.957 | Train PPL:  19.247\n",
      "\t Val. Loss: 3.837 |  Val. PPL:  46.382\n",
      "Epoch: 07 | Time: 0m 31s\n",
      "\tTrain Loss: 2.827 | Train PPL:  16.898\n",
      "\t Val. Loss: 3.701 |  Val. PPL:  40.492\n",
      "Epoch: 08 | Time: 0m 30s\n",
      "\tTrain Loss: 2.698 | Train PPL:  14.857\n",
      "\t Val. Loss: 3.591 |  Val. PPL:  36.259\n",
      "Epoch: 09 | Time: 0m 30s\n",
      "\tTrain Loss: 2.602 | Train PPL:  13.487\n",
      "\t Val. Loss: 3.630 |  Val. PPL:  37.707\n",
      "Epoch: 10 | Time: 0m 30s\n",
      "\tTrain Loss: 2.513 | Train PPL:  12.343\n",
      "\t Val. Loss: 3.664 |  Val. PPL:  39.011\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCN Incorporated\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, ninp, nembed, nhid, nlayers, dropout):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.embedding = nn.Embedding(ninp, nembed)\n",
    "        assert(nlayers > 0)\n",
    "        layers = [GCNLayer(nembed, nhid)] + [GCNLayer(nhid, nhid) for _ in range(nlayers-1)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.linear = nn.Linear(2*nhid, nhid)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, A):\n",
    "        \"\"\"\n",
    "        x: (seq len, b)\n",
    "        A: (b, seq len, seq len)\n",
    "        \"\"\"\n",
    "        x = x.t()\n",
    "        b = x.shape[0]\n",
    "        x = self.embedding(x)  # x: (b, seq len, ninp)\n",
    "        x = self.dropout(x)\n",
    "        hidden = []\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, A) \n",
    "            hidden.append(x[:,0,:])\n",
    "            \n",
    "        # pooling\n",
    "        mean = x.mean(dim=1)\n",
    "        maxm = x.max(dim=1)[0]\n",
    "        x = torch.cat((mean, maxm), dim=1)\n",
    "        out = self.linear(self.dropout(x))\n",
    "        hidden = torch.stack(hidden)\n",
    "        return out, hidden\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, grh, tgt, teacher_forcing_ratio=0.5):\n",
    "        # src: (src_len, b)\n",
    "        # tgt: (tgt_len, b)\n",
    "        tgt_len, b = tgt.shape\n",
    "        tgt_vocab_size = self.decoder.nout\n",
    "        \n",
    "        # tensor to store decoder outputs\n",
    "        outs = torch.zeros(tgt_len, b, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        enc_out, hidden = self.encoder(src, grh)\n",
    "        x = tgt[0]\n",
    "        for t in range(1, tgt_len):\n",
    "            out, hidden = self.decoder(x, hidden)\n",
    "            outs[t] = out\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = out.argmax(1)\n",
    "            x = tgt[t] if teacher_force else top1\n",
    "            \n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n",
      "Unique tokens in source (de) vocabulary: 7855\n",
      "Unique tokens in target (en) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "REVERSE = False\n",
    "SRC = Field(tokenize = lambda text: tokenize_de(text, REVERSE), \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "TGT = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "GRH = RawField(postprocessing=batch_graph)\n",
    "data_fields = [('src', SRC), ('trg', TGT), ('grh', GRH)]\n",
    "\n",
    "train_data = Dataset(torch.load(\"data/Multi30k/train_data.pt\"), data_fields)\n",
    "valid_data = Dataset(torch.load(\"data/Multi30k/valid_data.pt\"), data_fields)\n",
    "test_data = Dataset(torch.load(\"data/Multi30k/test_data.pt\"), data_fields)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    sort_key = lambda x: len(x.src),\n",
    "    sort_within_batch=False,\n",
    "    device = device)\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TGT.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TGT.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TGT.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 700\n",
    "DEC_HID_DIM = 700\n",
    "N_LAYERS = 3\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = GCNEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = GRUDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 17,692,681 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, batch in tqdm(enumerate(iterator)):\n",
    "        src = batch.src.to(device)\n",
    "        tgt = batch.trg.to(device)\n",
    "        grh = batch.grh.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(src, grh, tgt)\n",
    "        out_dim = out.shape[-1]\n",
    "        out = out[1:].view(-1, out_dim)\n",
    "        tgt = tgt[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(out, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss/len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm(enumerate(iterator)):\n",
    "            src = batch.src.to(device)\n",
    "            tgt = batch.trg.to(device)\n",
    "            grh = batch.grh.to(device)\n",
    "            out = model(src, grh, tgt, 0) # no teacher forcing here\n",
    "            out_dim = out.shape[-1]\n",
    "            out = out[1:].view(-1, out_dim)\n",
    "            tgt = tgt[1:].view(-1)\n",
    "            loss += criterion(out, tgt).item()\n",
    "    return loss/len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start, end):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:34,  6.53it/s]\n",
      "8it [00:00, 23.75it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 35s\n",
      "\tTrain Loss: 4.839 | Train PPL: 126.312\n",
      "\t Val. Loss: 4.743 |  Val. PPL: 114.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:35,  6.43it/s]\n",
      "8it [00:00, 22.96it/s]\n",
      "1it [00:00,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0m 35s\n",
      "\tTrain Loss: 4.181 | Train PPL:  65.412\n",
      "\t Val. Loss: 4.654 |  Val. PPL: 105.049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:35,  6.48it/s]\n",
      "8it [00:00, 23.75it/s]\n",
      "1it [00:00,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0m 35s\n",
      "\tTrain Loss: 4.000 | Train PPL:  54.601\n",
      "\t Val. Loss: 4.544 |  Val. PPL:  94.064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:35,  6.42it/s]\n",
      "8it [00:00, 23.54it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0m 35s\n",
      "\tTrain Loss: 3.843 | Train PPL:  46.650\n",
      "\t Val. Loss: 4.494 |  Val. PPL:  89.501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:35,  6.45it/s]\n",
      "8it [00:00, 23.30it/s]\n",
      "1it [00:00,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0m 35s\n",
      "\tTrain Loss: 3.761 | Train PPL:  42.976\n",
      "\t Val. Loss: 4.446 |  Val. PPL:  85.265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:35,  6.45it/s]\n",
      "8it [00:00, 23.37it/s]\n",
      "1it [00:00,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Time: 0m 35s\n",
      "\tTrain Loss: 3.655 | Train PPL:  38.661\n",
      "\t Val. Loss: 4.479 |  Val. PPL:  88.132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:35,  6.44it/s]\n",
      "8it [00:00, 23.43it/s]\n",
      "1it [00:00,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Time: 0m 35s\n",
      "\tTrain Loss: 3.587 | Train PPL:  36.115\n",
      "\t Val. Loss: 4.461 |  Val. PPL:  86.611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:35,  6.42it/s]\n",
      "8it [00:00, 23.79it/s]\n",
      "1it [00:00,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Time: 0m 35s\n",
      "\tTrain Loss: 3.503 | Train PPL:  33.217\n",
      "\t Val. Loss: 4.475 |  Val. PPL:  87.828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:35,  6.45it/s]\n",
      "8it [00:00, 23.57it/s]\n",
      "1it [00:00,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Time: 0m 35s\n",
      "\tTrain Loss: 3.465 | Train PPL:  31.980\n",
      "\t Val. Loss: 4.474 |  Val. PPL:  87.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:35,  6.47it/s]\n",
      "8it [00:00, 23.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Time: 0m 35s\n",
      "\tTrain Loss: 3.390 | Train PPL:  29.670\n",
      "\t Val. Loss: 4.441 |  Val. PPL:  84.826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "TGT_PAD_IDX = TGT.vocab.stoi[TGT.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TGT_PAD_IDX)\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCN and GRU Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNGRUEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, ninp, nembed, enc_nhid, dec_nhid, nlayers, dropout, device):\n",
    "        super(GCNGRUEncoder, self).__init__()\n",
    "        self.enc_nhid = enc_nhid\n",
    "        self.dec_nhid = dec_nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.embedding = nn.Embedding(ninp, nembed)\n",
    "        self.device = device\n",
    "        rnns = [nn.GRU(nembed, enc_nhid, 1, bidirectional=True)] + \\\n",
    "               [nn.GRU(enc_nhid*2, enc_nhid, 1, bidirectional=True) \n",
    "                for _ in range(nlayers-1)]\n",
    "        self.rnns = nn.ModuleList(rnns)\n",
    "        layers = [GCNLayer(nembed, enc_nhid)] + [GCNLayer(enc_nhid, enc_nhid) \n",
    "                                                   for _ in range(nlayers-1)]\n",
    "        self.gcns = nn.ModuleList(layers)\n",
    "        self.proj = nn.Linear(enc_nhid*2, enc_nhid)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, A):\n",
    "        \"\"\"\n",
    "        x: (seq len, b)\n",
    "        A: (b, seq len, seq len)\n",
    "        \"\"\"\n",
    "        s, b = x.shape\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        hiddens = []\n",
    "        hidden = torch.zeros(2, b, self.enc_nhid).to(self.device)\n",
    "        gcn_out, gru_out = embedded.transpose(1,0), embedded\n",
    "        for i in range(self.nlayers):\n",
    "            gcn_out = self.gcns[i](gcn_out, A)\n",
    "            gru_out, hidden = self.rnns[i](gru_out, hidden)\n",
    "            gru_out = self.dropout(gru_out)            \n",
    "            gru_out = gcn_out.repeat(1,1,2).transpose(0,1) + gru_out\n",
    "            gcn_out = self.proj(gru_out.transpose(0,1))\n",
    "            hidden += gcn_out.max(1)[0].repeat(2,1,1)\n",
    "            hiddens.append(hidden.sum(dim=0))\n",
    "        hidden = torch.stack(hiddens)\n",
    "        return gru_out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TGT.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "N_LAYERS = 3\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "enc = GCNGRUEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, N_LAYERS, ENC_DROPOUT, device)\n",
    "dec = GRUDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVERSE = False\n",
    "SRC = Field(tokenize = lambda text: tokenize_de(text, REVERSE), \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "TGT = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "GRH = RawField(postprocessing=batch_graph)\n",
    "data_fields = [('src', SRC), ('trg', TGT), ('grh', GRH)]\n",
    "\n",
    "train_data = Dataset(torch.load(\"data/Multi30k/train_data.pt\"), data_fields)\n",
    "valid_data = Dataset(torch.load(\"data/Multi30k/valid_data.pt\"), data_fields)\n",
    "test_data = Dataset(torch.load(\"data/Multi30k/test_data.pt\"), data_fields)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    sort_key = lambda x: len(x.src),\n",
    "    sort_within_batch=False,\n",
    "    device = device)\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TGT.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TGT.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "TGT_PAD_IDX = TGT.vocab.stoi[TGT.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TGT_PAD_IDX)\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCN Attention Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.gru_attn import Attention, GRUDecoder\n",
    "\n",
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, ninp, nembed, enc_nhid, dec_nhid, nlayers, dropout):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.enc_nhid = enc_nhid\n",
    "        self.dec_nhid = dec_nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.embedding = nn.Embedding(ninp, nembed)\n",
    "        assert(nlayers > 0)\n",
    "        layers = [GCNLayer(nembed, enc_nhid)] + \\\n",
    "                 [GCNLayer(enc_nhid, enc_nhid) for _ in range(nlayers-1)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.out_linear = nn.Linear(enc_nhid, 2*enc_nhid)\n",
    "        self.hid_linear = nn.Linear(enc_nhid, dec_nhid)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, A):\n",
    "        \"\"\"\n",
    "        x: (seq len, b)\n",
    "        A: (b, seq len, seq len)\n",
    "        \"\"\"\n",
    "        x = x.t()\n",
    "        b = x.shape[0]\n",
    "        x = self.embedding(x)  # x: (b, seq len, ninp)\n",
    "        x = self.dropout(x)\n",
    "        hidden = []\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, A) \n",
    "            hidden.append(x[:,0,:])\n",
    "            \n",
    "        # pooling\n",
    "        out = self.out_linear(self.dropout(x)).transpose(1,0)\n",
    "        hidden = torch.stack(hidden)\n",
    "        hidden = self.hid_linear(hidden)\n",
    "        return out, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(GCN2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, grh, tgt, teacher_forcing_ratio=0.5):\n",
    "        # src: (src_len, b)\n",
    "        # tgt: (tgt_len, b)\n",
    "        tgt_len, b = tgt.shape\n",
    "        tgt_vocab_size = self.decoder.nout\n",
    "        \n",
    "        # tensor to store decoder outputs\n",
    "        outs = torch.zeros(tgt_len, b, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        enc_out, hidden = self.encoder(src, grh)\n",
    "        x = tgt[0]\n",
    "        attns = []\n",
    "        for t in range(1, tgt_len):\n",
    "            out, hidden, attn = self.decoder(x, hidden, enc_out)\n",
    "            attns.append(attn)\n",
    "            outs[t] = out\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = out.argmax(1)\n",
    "            x = tgt[t] if teacher_force else top1\n",
    "        attns = torch.stack(attns, dim=1).squeeze()\n",
    "        return outs, attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n",
      "Unique tokens in source (de) vocabulary: 7855\n",
      "Unique tokens in target (en) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "REVERSE = False\n",
    "SRC = Field(tokenize = lambda text: tokenize_de(text, REVERSE), \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "TGT = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "GRH = RawField(postprocessing=batch_graph)\n",
    "data_fields = [('src', SRC), ('trg', TGT), ('grh', GRH)]\n",
    "\n",
    "train_data = Dataset(torch.load(\"data/Multi30k/train_data.pt\"), data_fields)\n",
    "valid_data = Dataset(torch.load(\"data/Multi30k/valid_data.pt\"), data_fields)\n",
    "test_data = Dataset(torch.load(\"data/Multi30k/test_data.pt\"), data_fields)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    sort_key = lambda x: len(x.src),\n",
    "    sort_within_batch=False,\n",
    "    device = device)\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TGT.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TGT.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TGT.vocab)\n",
    "ENC_EMB_DIM = 250\n",
    "DEC_EMB_DIM = 250\n",
    "ENC_HID_DIM = 500\n",
    "DEC_HID_DIM = 500\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "NLAYERS = 2\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = GCNEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, NLAYERS, ENC_DROPOUT)\n",
    "dec = GRUDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, NLAYERS, DEC_DROPOUT, attn)\n",
    "model = GCN2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 19,765,143 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, batch in tqdm(enumerate(iterator)):\n",
    "        src = batch.src.to(device)\n",
    "        tgt = batch.trg.to(device)\n",
    "        grh = batch.grh.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out, attns = model(src, grh, tgt)\n",
    "        out_dim = out.shape[-1]\n",
    "        out = out[1:].view(-1, out_dim)\n",
    "        tgt = tgt[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(out, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss/len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm(enumerate(iterator)):\n",
    "            src = batch.src.to(device)\n",
    "            tgt = batch.trg.to(device)\n",
    "            grh = batch.grh.to(device)\n",
    "            out, attns = model(src, grh, tgt, 0) # no teacher forcing here\n",
    "            out_dim = out.shape[-1]\n",
    "            out = out[1:].view(-1, out_dim)\n",
    "            tgt = tgt[1:].view(-1)\n",
    "            loss += criterion(out, tgt).item()\n",
    "    return loss/len(iterator)\n",
    "\n",
    "def epoch_time(start, end):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1it [00:19, 19.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "TGT_PAD_IDX = TGT.vocab.stoi[TGT.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TGT_PAD_IDX)\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCNGRU Attention Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baby test for GCN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in train_iterator:\n",
    "    x = b.src.to(device)\n",
    "    A = b.grh.to(device)\n",
    "    break\n",
    "\n",
    "out, hidden = enc(x, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 512])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
