{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import spacy, random, math, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.datasets import TranslationDataset, Multi30k, IWSLT\n",
    "from torchtext.data import Field, BucketIterator, RawField, Dataset\n",
    "\n",
    "from models.gcn import GCNLayer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi30k\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 11747\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text, reverse=False):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    tokens = [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "    return tokens[::-1] if reverse else tokens\n",
    "    \n",
    "def tokenize_en(text, reverse=False):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    tokens = [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "    return tokens[::-1] if reverse else tokens\n",
    "    \n",
    "def batch_graph(grhs):\n",
    "    \"\"\" batch a list of graphs\n",
    "    @param grhs: list(tensor,...) \n",
    "    \"\"\"\n",
    "    b = len(grhs)  # batch size\n",
    "    graph_dims = [len(g) for g in grhs]\n",
    "    s = max(graph_dims)  # max seq length\n",
    "    \n",
    "    G = torch.zeros([b, s, s])\n",
    "    for i, g in enumerate(grhs):\n",
    "        s_ = graph_dims[i]\n",
    "        G[i,:s_,:s_] = g\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVERSE = True\n",
    "SRC = Field(tokenize = lambda text: tokenize_de(text, REVERSE), \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "TGT = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "GRH = RawField(postprocessing=batch_graph)\n",
    "data_fields = [('src', SRC), ('trg', TGT), ('grh', GRH)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset(torch.load(\"data/Multi30k/train_data.pt\"), data_fields)\n",
    "valid_data = Dataset(torch.load(\"data/Multi30k/valid_data.pt\"), data_fields)\n",
    "test_data = Dataset(torch.load(\"data/Multi30k/test_data.pt\"), data_fields)\n",
    "\n",
    "# no-graph data\n",
    "# train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "#                                                     fields = (SRC, TGT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    sort_key = lambda x: len(x.src),\n",
    "    sort_within_batch=False,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n",
      "Unique tokens in source (de) vocabulary: 7855\n",
      "Unique tokens in target (en) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TGT.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TGT.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_sentence_lengths(dataset):\n",
    "    src_counter = Counter()\n",
    "    tgt_counter = Counter()\n",
    "    for exp in dataset:\n",
    "        src_counter[len(exp.src)] += 1\n",
    "        tgt_counter[len(exp.trg)] += 1\n",
    "    return src_counter, tgt_counter\n",
    "\n",
    "def counter2array(counter):\n",
    "    result = []\n",
    "    for k in counter:\n",
    "        result.extend([k for _ in range(counter[k])])\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum src, tgt sent lengths: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(44, 41)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_c, tgt_c = get_sentence_lengths(train_data)\n",
    "src_lengths = counter2array(src_c)\n",
    "tgt_lengths = counter2array(tgt_c)\n",
    "\n",
    "print(\"maximum src, tgt sent lengths: \")\n",
    "np.quantile(src_lengths, 1), np.quantile(tgt_lengths, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment with just GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, ninp, nembed, nhid, nlayers, dropout):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.embedding = nn.Embedding(ninp, nembed)\n",
    "        assert(nlayers > 0)\n",
    "        layers = [GCNLayer(nembed, nhid)] + [GCNLayer(nhid, nhid) for _ in range(nlayers-1)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.linear = nn.Linear(2*nhid, nhid)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, A):\n",
    "        \"\"\"\n",
    "        x: (b, seq len)\n",
    "        A: (b, seq len, seq len)\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)  # x: (b, seq len, ninp)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, A)\n",
    "            \n",
    "        # pooling\n",
    "        mean = x.mean(dim=1)\n",
    "        maxm = x.max(dim=1)[0]\n",
    "        x = torch.cat((mean, maxm), dim=1)\n",
    "        out = self.linear(self.dropout(x))\n",
    "        return out\n",
    "    \n",
    "class GRUEncoder(nn.Module):\n",
    "    def __init__(self, ninp, nembed, enc_nhid, dec_nhid, nlayers, dropout):\n",
    "        super(GRUEncoder, self).__init__()\n",
    "        self.enc_nhid = enc_nhid\n",
    "        self.dec_nhid = dec_nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.embedding = nn.Embedding(ninp, nembed)\n",
    "        self.rnn = nn.GRU(nembed, enc_nhid, nlayers, bidirectional=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(enc_nhid*2, dec_nhid)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src: (src len, b)\n",
    "        s, b = src.shape\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        out, hidden = self.rnn(embedded)\n",
    "        hidden = hidden.transpose(1,0).reshape(b, self.nlayers, -1).transpose(1,0)\n",
    "        hidden = torch.tanh(self.fc(hidden))\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUDecoder(nn.Module):\n",
    "    def __init__(self, nout, nembed, enc_nhid, dec_nhid, nlayers, dropout):\n",
    "        super(GRUDecoder, self).__init__()\n",
    "        self.nout = nout\n",
    "        self.enc_nhid = enc_nhid\n",
    "        self.dec_nhid = dec_nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.embedding = nn.Embedding(nout, nembed)\n",
    "        self.rnn = nn.GRU(nembed, dec_nhid, nlayers, bidirectional=False, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(dec_nhid, nout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = x.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        out, hidden = self.rnn(embedded, hidden)\n",
    "        pred = self.fc_out(out.squeeze(0))\n",
    "        return pred, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        # src: (src_len, b)\n",
    "        # tgt: (tgt_len, b)\n",
    "        tgt_len, b = tgt.shape\n",
    "        tgt_vocab_size = self.decoder.nout\n",
    "        \n",
    "        # tensor to store decoder outputs\n",
    "        outs = torch.zeros(tgt_len, b, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        enc_out, hidden = self.encoder(src)\n",
    "        x = tgt[0]\n",
    "        for t in range(1, tgt_len):\n",
    "            out, hidden = self.decoder(x, hidden)\n",
    "            outs[t] = out\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = out.argmax(1)\n",
    "            x = tgt[t] if teacher_force else top1\n",
    "            \n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TGT.vocab)\n",
    "ENC_EMB_DIM = 250\n",
    "DEC_EMB_DIM = 250\n",
    "ENC_HID_DIM = 500\n",
    "DEC_HID_DIM = 500\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "NLAYERS = 2\n",
    "\n",
    "enc = GRUEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, NLAYERS, ENC_DROPOUT)\n",
    "dec = GRUDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, NLAYERS, DEC_DROPOUT)\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): GRUEncoder(\n",
       "    (embedding): Embedding(7855, 250)\n",
       "    (rnn): GRU(250, 500, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "    (fc): Linear(in_features=1000, out_features=500, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): GRUDecoder(\n",
       "    (embedding): Embedding(5893, 250)\n",
       "    (rnn): GRU(250, 500, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=500, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 16,282,893 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "TGT_PAD_IDX = TGT.vocab.stoi[TGT.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TGT_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        tgt = batch.trg\n",
    "        optimizer.zero_grad()\n",
    "        out = model(src, tgt)\n",
    "        out_dim = out.shape[-1]\n",
    "        out = out[1:].view(-1, out_dim)\n",
    "        tgt = tgt[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(out, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss/len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            tgt = batch.trg\n",
    "            out = model(src, tgt, 0) # no teacher forcing here\n",
    "            out_dim = out.shape[-1]\n",
    "            out = out[1:].view(-1, out_dim)\n",
    "            tgt = tgt[1:].view(-1)\n",
    "            loss += criterion(out, tgt).item()\n",
    "    return loss/len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start, end):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 26s\n",
      "\tTrain Loss: 4.754 | Train PPL: 116.086\n",
      "\t Val. Loss: 4.587 |  Val. PPL:  98.205\n",
      "Epoch: 02 | Time: 1m 26s\n",
      "\tTrain Loss: 3.958 | Train PPL:  52.371\n",
      "\t Val. Loss: 4.364 |  Val. PPL:  78.591\n",
      "Epoch: 03 | Time: 1m 26s\n",
      "\tTrain Loss: 3.552 | Train PPL:  34.890\n",
      "\t Val. Loss: 4.117 |  Val. PPL:  61.394\n",
      "Epoch: 04 | Time: 1m 26s\n",
      "\tTrain Loss: 3.308 | Train PPL:  27.336\n",
      "\t Val. Loss: 4.021 |  Val. PPL:  55.750\n",
      "Epoch: 05 | Time: 1m 25s\n",
      "\tTrain Loss: 3.115 | Train PPL:  22.532\n",
      "\t Val. Loss: 3.755 |  Val. PPL:  42.727\n",
      "Epoch: 06 | Time: 1m 26s\n",
      "\tTrain Loss: 2.937 | Train PPL:  18.850\n",
      "\t Val. Loss: 3.663 |  Val. PPL:  38.986\n",
      "Epoch: 07 | Time: 1m 27s\n",
      "\tTrain Loss: 2.777 | Train PPL:  16.071\n",
      "\t Val. Loss: 3.615 |  Val. PPL:  37.160\n",
      "Epoch: 08 | Time: 1m 27s\n",
      "\tTrain Loss: 2.679 | Train PPL:  14.577\n",
      "\t Val. Loss: 3.602 |  Val. PPL:  36.689\n",
      "Epoch: 09 | Time: 1m 26s\n",
      "\tTrain Loss: 2.575 | Train PPL:  13.125\n",
      "\t Val. Loss: 3.572 |  Val. PPL:  35.599\n",
      "Epoch: 10 | Time: 1m 25s\n",
      "\tTrain Loss: 2.465 | Train PPL:  11.763\n",
      "\t Val. Loss: 3.620 |  Val. PPL:  37.352\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TGT.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.2\n",
    "DEC_DROPOUT = 0.2\n",
    "\n",
    "encoder = GCNEncoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 512])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.layers[0].W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in train_iterator:\n",
    "    x = b.src.t()\n",
    "    A = b.grh\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
