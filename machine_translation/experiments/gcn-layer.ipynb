{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, random, math, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.datasets import TranslationDataset, Multi30k, IWSLT\n",
    "from torchtext.data import Field, BucketIterator, RawField, Dataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment with just GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout = 0.2):\n",
    "        \"\"\"\n",
    "        each layer has the following form of computation\n",
    "        H = f(A * H * W)\n",
    "        H: (b, seq len, ninp)\n",
    "        A: (b, seq len, seq len)\n",
    "        W: (ninp, nout)\n",
    "        \"\"\"\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(input_dim, output_dim))\n",
    "        self.b = nn.Parameter(torch.randn(output_dim))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, A):\n",
    "        \"\"\"\n",
    "        H = relu(A * x * W)\n",
    "        x: (b, seq len, ninp)\n",
    "        A: (b, seq len, seq len)\n",
    "        W: (ninp, nout)\n",
    "        \"\"\"\n",
    "        x = self.dropout(x)\n",
    "        x = torch.bmm(A, x)  # x: (b, seq len, ninp)\n",
    "        x = x.matmul(self.W) + self.b\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baby Test\n",
    "\n",
    "From the test, we know the right way to batch matrix is:\n",
    "1. align the batch dim\n",
    "2. with in the same batch, pad smaller matrix on the right and bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCNLayer(\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = GCNLayer(4, 2, 0)\n",
    "layer1.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "A = torch.tensor([[[1,0,0],\n",
    "                   [1,1,1], \n",
    "                   [0,1,1]],\n",
    "                  [[1,0,0],\n",
    "                   [1,1,0], \n",
    "                   [0,0,0]]], dtype=torch.float)\n",
    "x = torch.tensor([[[1,2,3,4],\n",
    "                   [4,5,6,7], \n",
    "                   [7,8,9,8]],\n",
    "                  [[100, 200, 300, 400],\n",
    "                   [200, 300, 400, 500], \n",
    "                   [0, 0, 0, 0]]], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000e+00, 9.1991e+00],\n",
       "         [0.0000e+00, 4.6133e+01],\n",
       "         [0.0000e+00, 3.6460e+01]],\n",
       "\n",
       "        [[0.0000e+00, 9.6678e+02],\n",
       "         [0.0000e+00, 2.1880e+03],\n",
       "         [1.3163e+00, 0.0000e+00]]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1(x, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4.],\n",
       "        [4., 5., 6., 7.],\n",
       "        [7., 8., 9., 8.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1265,  0.0858],\n",
       "        [ 0.5089,  0.4087],\n",
       "        [ 0.9336, -0.5888],\n",
       "        [-3.0925,  2.6340]], requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.4119e+02,  9.6678e+02],\n",
       "        [-1.8360e+03,  2.1880e+03],\n",
       "        [ 1.3163e+00, -4.7340e-01]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 1\n",
    "A[b].matmul(x[b]).matmul(layer1.W)+layer1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
