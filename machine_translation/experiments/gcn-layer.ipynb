{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, random, math, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.datasets import TranslationDataset, Multi30k, IWSLT\n",
    "from torchtext.data import Field, BucketIterator, RawField, Dataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment with just GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout = 0.2):\n",
    "        \"\"\"\n",
    "        each layer has the following form of computation\n",
    "        H = f(A * H * W)\n",
    "        H: (b, seq len, ninp)\n",
    "        A: (b, seq len, seq len)\n",
    "        W: (ninp, nout)\n",
    "        \"\"\"\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(input_dim, output_dim))\n",
    "        self.b = nn.Parameter(torch.randn(output_dim))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, A):\n",
    "        \"\"\"\n",
    "        H = relu(A * x * W)\n",
    "        x: (b, seq len, ninp)\n",
    "        A: (b, seq len, seq len)\n",
    "        W: (ninp, nout)\n",
    "        \"\"\"\n",
    "        x = self.dropout(x)\n",
    "        x = torch.bmm(A, x)  # x: (b, seq len, ninp)\n",
    "        x = x.matmul(self.W) + self.b\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baby Test\n",
    "\n",
    "From the test, we know the right way to batch matrix is:\n",
    "1. align the batch dim\n",
    "2. with in the same batch, pad smaller matrix on the right and bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCNLayer(\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = GCNLayer(3, 2, 0)\n",
    "layer1.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "A = torch.tensor([[[1,0,0],\n",
    "                   [1,1,1], \n",
    "                   [0,1,1]],\n",
    "                  [[1,0,0],\n",
    "                   [1,1,0], \n",
    "                   [0,0,0]]], dtype=torch.float)\n",
    "x = torch.tensor([[[1,2,3],\n",
    "                   [4,5,6], \n",
    "                   [7,8,9]],\n",
    "                  [[100, 200, 300],\n",
    "                   [200, 300, 400], \n",
    "                   [0, 0, 0]]], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3465,  1.7680],\n",
       "         [ 0.0000,  6.5830],\n",
       "         [ 0.0000,  7.0198]],\n",
       "\n",
       "        [[55.2780,  0.0000],\n",
       "         [75.6653,  0.0000],\n",
       "         [ 0.8018,  2.2048]]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1(x, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [7., 8., 9.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.5969,  0.5413],\n",
       "        [-0.3737,  1.2504],\n",
       "        [ 0.6297, -1.1596]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 55.2780, -41.4739],\n",
       "        [ 75.6653, -21.9457],\n",
       "        [  0.8018,   2.2048]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 1\n",
    "A[b].matmul(x[b]).matmul(layer1.W)+layer1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
