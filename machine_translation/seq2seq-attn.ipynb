{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k, IWSLT\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 11747\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TGT = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.en', '.de'), \n",
    "                                                    fields = (SRC, TGT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.'], 'trg': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (de) vocabulary: 5893\n",
      "Unique tokens in target (en) vocabulary: 7855\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TGT.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TGT.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple LSTM Encoder Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_nhid, dec_nhid):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(enc_nhid*2+dec_nhid, dec_nhid)\n",
    "        self.v = nn.Linear(dec_nhid, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, enc_out):\n",
    "        # hidden: (b, nhid)\n",
    "        # enc_out: (s, b, nhid*2)\n",
    "        b = enc_out.shape[1]\n",
    "        src_len = enc_out.shape[0]\n",
    "        hidden = hidden[-1].unsqueeze(1).repeat(1, src_len, 1)\n",
    "        enc_out = enc_out.permute(1, 0, 2)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, enc_out), dim=2)))\n",
    "        # energy: (b, s, nhid)\n",
    "        attn = self.v(energy).squeeze(2)\n",
    "        return F.softmax(attn, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, ninp, nembed, enc_nhid, dec_nhid, nlayers, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(ninp, nembed)\n",
    "        self.nlayers = nlayers\n",
    "        self.rnn = nn.GRU(nembed, enc_nhid, nlayers, dropout=dropout, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_nhid*2, dec_nhid)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src: (s, b)\n",
    "        s, b = src.shape\n",
    "        src = self.dropout(self.embedding(src))\n",
    "        # src: (s, b, nembed), hidden: (nlayers*dir, b, nhid), cell: (nlayers*dir, b, nhid)\n",
    "        out, hidden = self.rnn(src)\n",
    "        hidden = hidden.transpose(1,0).reshape(b, self.nlayers, -1).transpose(1,0)\n",
    "        hidden = torch.tanh(self.fc(hidden))\n",
    "        return out, hidden\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, nout, nembed, enc_nhid, dec_nhid, nlayers, dropout=0.2, attention=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.nout = nout\n",
    "        self.nlayers = nlayers\n",
    "        self.embedding = nn.Embedding(nout, nembed)\n",
    "        self.rnn = nn.GRU(nembed+enc_nhid*2, dec_nhid, nlayers, \n",
    "                          dropout=dropout, bidirectional=False)\n",
    "        self.fc_out = nn.Linear(enc_nhid*2+dec_nhid+nembed, nout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = attention\n",
    "    \n",
    "    def forward(self, src, hidden, enc_out):\n",
    "        \"\"\"\n",
    "        for decoder, we process one token at a time\n",
    "        \"\"\"\n",
    "        # src: (b, )\n",
    "        embedded = src.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(embedded))\n",
    "        attn = self.attention(hidden, enc_out)\n",
    "        attn = attn.unsqueeze(1)\n",
    "        enc_out = enc_out.permute(1,0,2)\n",
    "        ctxt = torch.bmm(attn, enc_out).permute(1,0,2)\n",
    "        x = torch.cat((embedded, ctxt), dim=2)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        # out, hidden: (1, b, nhid)\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        out = out.squeeze(0)\n",
    "        ctxt = ctxt.squeeze(0)\n",
    "        pred = self.fc_out(torch.cat((out, ctxt, embedded), dim=1))\n",
    "        return pred, hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        # src: (src_len, b)\n",
    "        # tgt: (tgt_len, b)\n",
    "        tgt_len, b = tgt.shape\n",
    "        tgt_vocab_size = self.decoder.nout\n",
    "        \n",
    "        # tensor to store decoder outputs\n",
    "        outs = torch.zeros(tgt_len, b, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        enc_out, hidden = self.encoder(src)\n",
    "        x = tgt[0]\n",
    "        for t in range(1, tgt_len):\n",
    "            out, hidden = self.decoder(x, hidden, enc_out)\n",
    "            outs[t] = out\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = out.argmax(1)\n",
    "            x = tgt[t] if teacher_force else top1\n",
    "            \n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TGT.vocab)\n",
    "ENC_EMB_DIM = 250\n",
    "DEC_EMB_DIM = 250\n",
    "ENC_HID_DIM = 500\n",
    "DEC_HID_DIM = 500\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "NLAYERS = 2\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, NLAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, NLAYERS, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(5893, 250)\n",
       "    (rnn): GRU(250, 500, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (fc): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(7855, 250)\n",
       "    (rnn): GRU(1250, 500, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=1750, out_features=7855, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=1500, out_features=500, bias=True)\n",
       "      (v): Linear(in_features=500, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 29,335,605 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "TGT_PAD_IDX = TGT.vocab.stoi[TGT.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TGT_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        tgt = batch.trg\n",
    "        optimizer.zero_grad()\n",
    "        out = model(src, tgt)\n",
    "        out_dim = out.shape[-1]\n",
    "        out = out[1:].view(-1, out_dim)\n",
    "        tgt = tgt[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(out, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss/len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            tgt = batch.trg\n",
    "            out = model(src, tgt, 0) # no teacher forcing here\n",
    "            out_dim = out.shape[-1]\n",
    "            out = out[1:].view(-1, out_dim)\n",
    "            tgt = tgt[1:].view(-1)\n",
    "            loss += criterion(out, tgt).item()\n",
    "    return loss/len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start, end):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 4m 23s\n",
      "\tTrain Loss: 4.612 | Train PPL: 100.677\n",
      "\t Val. Loss: 4.135 |  Val. PPL:  62.502\n",
      "Epoch: 02 | Time: 4m 23s\n",
      "\tTrain Loss: 3.308 | Train PPL:  27.320\n",
      "\t Val. Loss: 3.495 |  Val. PPL:  32.962\n",
      "Epoch: 03 | Time: 4m 21s\n",
      "\tTrain Loss: 2.644 | Train PPL:  14.067\n",
      "\t Val. Loss: 3.437 |  Val. PPL:  31.102\n",
      "Epoch: 04 | Time: 4m 23s\n",
      "\tTrain Loss: 2.228 | Train PPL:   9.284\n",
      "\t Val. Loss: 3.370 |  Val. PPL:  29.088\n",
      "Epoch: 05 | Time: 4m 21s\n",
      "\tTrain Loss: 1.937 | Train PPL:   6.941\n",
      "\t Val. Loss: 3.389 |  Val. PPL:  29.648\n",
      "Epoch: 06 | Time: 4m 22s\n",
      "\tTrain Loss: 1.764 | Train PPL:   5.834\n",
      "\t Val. Loss: 3.379 |  Val. PPL:  29.337\n",
      "Epoch: 07 | Time: 4m 22s\n",
      "\tTrain Loss: 1.604 | Train PPL:   4.975\n",
      "\t Val. Loss: 3.532 |  Val. PPL:  34.190\n",
      "Epoch: 08 | Time: 4m 23s\n",
      "\tTrain Loss: 1.490 | Train PPL:   4.439\n",
      "\t Val. Loss: 3.575 |  Val. PPL:  35.694\n",
      "Epoch: 09 | Time: 4m 21s\n",
      "\tTrain Loss: 1.395 | Train PPL:   4.037\n",
      "\t Val. Loss: 3.625 |  Val. PPL:  37.538\n",
      "Epoch: 10 | Time: 4m 22s\n",
      "\tTrain Loss: 1.328 | Train PPL:   3.775\n",
      "\t Val. Loss: 3.704 |  Val. PPL:  40.597\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
