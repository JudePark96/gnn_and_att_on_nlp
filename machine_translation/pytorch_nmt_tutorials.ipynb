{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchnlp.datasets import wmt_dataset, iwslt_dataset  # doctest: +SKIP\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the iwslt dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = iwslt_dataset(train=True, dev=True, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(dataset, reverse=False):\n",
    "    \"\"\" \n",
    "    @param dataset: list of dicts\n",
    "        eg. [{'en': 'aaaaa', 'de': 'bbbbb'}, ....]\"\"\"\n",
    "    print(\"Reading lines...\", end='')\n",
    "    \n",
    "    lang1, lang2 = dataset[0].keys()\n",
    "    \n",
    "    Lang1, Lang2 = Lang(lang1), Lang(lang2)\n",
    "    pairs = []\n",
    "    for pair in dataset:\n",
    "        if not reverse:\n",
    "            Lang1.addSentence(pair[lang1])\n",
    "            Lang2.addSentence(pair[lang2])\n",
    "            pairs.append((pair[lang1], pair[lang2]))\n",
    "        else:\n",
    "            Lang1.addSentence(pair[lang2])\n",
    "            Lang2.addSentence(pair[lang1])\n",
    "            pairs.append((pair[lang2], pair[lang1]))\n",
    "    print(\"complete!\")\n",
    "    return Lang1, Lang2, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(dataset, reverse=False):\n",
    "    source_lang, target_lang, pairs = readLangs(dataset, reverse)\n",
    "    print(\"n words total: \")\n",
    "    print(\"Source lang: \", source_lang.name, source_lang.n_words)\n",
    "    print(\"Target lang: \", target_lang.name, target_lang.n_words)\n",
    "    return source_lang, target_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...complete!\n",
      "n words total: \n",
      "Source lang:  en 123158\n",
      "Target lang:  de 207319\n",
      "CPU times: user 3.67 s, sys: 72.3 ms, total: 3.75 s\n",
      "Wall time: 3.76 s\n"
     ]
    }
   ],
   "source": [
    "%time source_lang, target_lang, train_pairs = prepareData(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, ninp, nhid):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.nhid = nhid\n",
    "        self.embedding = nn.Embedding(ninp, nhid)\n",
    "        self.gru = nn.GRU(nhid, nhid)\n",
    "        \n",
    "    def forward(self, src, hidden):\n",
    "        embed = self.embedding(src).view(1,1,-1)\n",
    "        out, hidden = self.gru(embed, hidden)\n",
    "        return out, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size, device=device)\n",
    "    \n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, nhid, nout):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.nhid = nhid\n",
    "        self.embedding = nn.Embedding(nout, nhid)\n",
    "        self.gru = nn.GRU(nhid, nhid)\n",
    "        self.out = nn.Linear(nhid, nout)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, src, hidden):\n",
    "        out = self.embedding(src).view(1,1,-1)\n",
    "        out = F.relu(out)\n",
    "        out, hidden = self.gru(out, hidden)\n",
    "        out = self.softmax(self.out(out[0]))\n",
    "        \n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, nhid, nout, dropout=0.2, max_length=100):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.nhid = nhid\n",
    "        self.nout = nout\n",
    "        self.dropout = dropout\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.nout, self.nhid)\n",
    "        self.attn = nn.Linear(self.nhid*2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size*2, self.nhid)\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.gru = nn.GRU(self.nhid, self.nhid)\n",
    "        self.out = nn.Linear(self.nhid, self.nout)\n",
    "        \n",
    "    def forward(self, src, hidden, encoder_outputs):\n",
    "        embed = self.embedding(src).view(1,1,-1)\n",
    "        embed = self.dropout(embed)\n",
    "        \n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embed[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        output = torch.cat((embed[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        out, hidden = self.gru(output, hidden)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1,self.nhid, device=device)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indixes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_TOKEN)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1,1)\n",
    "\n",
    "def tensorsFromPair(pair, src_lang, tgt_lang):\n",
    "    src = tensorFromSentence(src_lang, pair[0])\n",
    "    tgt = tensorFromSentence(tgt_lang, pair[1])\n",
    "    return (src, tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(src, tgt, encoder, decoder, encoder_optimizer, \n",
    "          decoder_optimizer, criterion, max_length=100):\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
