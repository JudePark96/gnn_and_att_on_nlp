{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import spacy, random, math, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.datasets import TranslationDataset, Multi30k, IWSLT\n",
    "from torchtext.data import Field, BucketIterator, RawField, Dataset\n",
    "\n",
    "from models.gcn import GCNLayer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi30k\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 11747\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text, reverse=False):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    tokens = [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "    return tokens[::-1] if reverse else tokens\n",
    "    \n",
    "def tokenize_en(text, reverse=False):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    tokens = [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "    return tokens[::-1] if reverse else tokens\n",
    "    \n",
    "def batch_graph(grhs):\n",
    "    \"\"\" batch a list of graphs\n",
    "    @param grhs: list(tensor,...) \n",
    "    \"\"\"\n",
    "    b = len(grhs)  # batch size\n",
    "    graph_dims = [len(g) for g in grhs]\n",
    "    s = max(graph_dims)  # max seq length\n",
    "    \n",
    "    G = torch.zeros([b, s, s])\n",
    "    for i, g in enumerate(grhs):\n",
    "        s_ = graph_dims[i]\n",
    "        G[i,:s_,:s_] = g\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVERSE = True\n",
    "SRC = Field(tokenize = lambda text: tokenize_de(text, REVERSE), \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "TGT = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "GRH = RawField(postprocessing=batch_graph)\n",
    "data_fields = [('src', SRC), ('trg', TGT), ('grh', GRH)]\n",
    "\n",
    "train_data = Dataset(torch.load(\"data/Multi30k/train_data.pt\"), data_fields)\n",
    "valid_data = Dataset(torch.load(\"data/Multi30k/valid_data.pt\"), data_fields)\n",
    "test_data = Dataset(torch.load(\"data/Multi30k/test_data.pt\"), data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    sort_key = lambda x: len(x.src),\n",
    "    sort_within_batch=False,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n",
      "Unique tokens in source (de) vocabulary: 7855\n",
      "Unique tokens in target (en) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TGT.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TGT.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_sentence_lengths(dataset):\n",
    "    src_counter = Counter()\n",
    "    tgt_counter = Counter()\n",
    "    for exp in dataset:\n",
    "        src_counter[len(exp.src)] += 1\n",
    "        tgt_counter[len(exp.trg)] += 1\n",
    "    return src_counter, tgt_counter\n",
    "\n",
    "def counter2array(counter):\n",
    "    result = []\n",
    "    for k in counter:\n",
    "        result.extend([k for _ in range(counter[k])])\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum src, tgt sent lengths: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(44, 41)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_c, tgt_c = get_sentence_lengths(train_data)\n",
    "src_lengths = counter2array(src_c)\n",
    "tgt_lengths = counter2array(tgt_c)\n",
    "\n",
    "print(\"maximum src, tgt sent lengths: \")\n",
    "np.quantile(src_lengths, 1), np.quantile(tgt_lengths, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment with just GRU\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUEncoder(nn.Module):\n",
    "    def __init__(self, ninp, nembed, enc_nhid, dec_nhid, nlayers, dropout):\n",
    "        super(GRUEncoder, self).__init__()\n",
    "        self.enc_nhid = enc_nhid\n",
    "        self.dec_nhid = dec_nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.embedding = nn.Embedding(ninp, nembed)\n",
    "        self.rnn = nn.GRU(nembed, enc_nhid, nlayers, bidirectional=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(enc_nhid*2, dec_nhid)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src: (src len, b)\n",
    "        s, b = src.shape\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        out, hidden = self.rnn(embedded)\n",
    "        hidden = hidden.transpose(1,0).reshape(b, self.nlayers, -1).transpose(1,0)\n",
    "        hidden = torch.tanh(self.fc(hidden))\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUDecoder(nn.Module):\n",
    "    def __init__(self, nout, nembed, enc_nhid, dec_nhid, nlayers, dropout):\n",
    "        super(GRUDecoder, self).__init__()\n",
    "        self.nout = nout\n",
    "        self.enc_nhid = enc_nhid\n",
    "        self.dec_nhid = dec_nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.embedding = nn.Embedding(nout, nembed)\n",
    "        self.rnn = nn.GRU(nembed, dec_nhid, nlayers, bidirectional=False, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(dec_nhid, nout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = x.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        out, hidden = self.rnn(embedded, hidden)\n",
    "        pred = self.fc_out(out.squeeze(0))\n",
    "        return pred, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        # src: (src_len, b)\n",
    "        # tgt: (tgt_len, b)\n",
    "        tgt_len, b = tgt.shape\n",
    "        tgt_vocab_size = self.decoder.nout\n",
    "        \n",
    "        # tensor to store decoder outputs\n",
    "        outs = torch.zeros(tgt_len, b, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        enc_out, hidden = self.encoder(src)\n",
    "        x = tgt[0]\n",
    "        for t in range(1, tgt_len):\n",
    "            out, hidden = self.decoder(x, hidden)\n",
    "            outs[t] = out\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = out.argmax(1)\n",
    "            x = tgt[t] if teacher_force else top1\n",
    "            \n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TGT.vocab)\n",
    "ENC_EMB_DIM = 250\n",
    "DEC_EMB_DIM = 250\n",
    "ENC_HID_DIM = 500\n",
    "DEC_HID_DIM = 500\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "NLAYERS = 2\n",
    "\n",
    "enc = GRUEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, NLAYERS, ENC_DROPOUT)\n",
    "dec = GRUDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, NLAYERS, DEC_DROPOUT)\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): GRUEncoder(\n",
       "    (embedding): Embedding(7855, 250)\n",
       "    (rnn): GRU(250, 500, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "    (fc): Linear(in_features=1000, out_features=500, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): GRUDecoder(\n",
       "    (embedding): Embedding(5893, 250)\n",
       "    (rnn): GRU(250, 500, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=500, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 16,282,893 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "TGT_PAD_IDX = TGT.vocab.stoi[TGT.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TGT_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        tgt = batch.trg\n",
    "        optimizer.zero_grad()\n",
    "        out = model(src, tgt)\n",
    "        out_dim = out.shape[-1]\n",
    "        out = out[1:].view(-1, out_dim)\n",
    "        tgt = tgt[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(out, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss/len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            tgt = batch.trg\n",
    "            out = model(src, tgt, 0) # no teacher forcing here\n",
    "            out_dim = out.shape[-1]\n",
    "            out = out[1:].view(-1, out_dim)\n",
    "            tgt = tgt[1:].view(-1)\n",
    "            loss += criterion(out, tgt).item()\n",
    "    return loss/len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start, end):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 30s\n",
      "\tTrain Loss: 4.789 | Train PPL: 120.187\n",
      "\t Val. Loss: 4.878 |  Val. PPL: 131.371\n",
      "Epoch: 02 | Time: 0m 30s\n",
      "\tTrain Loss: 3.957 | Train PPL:  52.298\n",
      "\t Val. Loss: 4.547 |  Val. PPL:  94.388\n",
      "Epoch: 03 | Time: 0m 30s\n",
      "\tTrain Loss: 3.582 | Train PPL:  35.931\n",
      "\t Val. Loss: 4.202 |  Val. PPL:  66.814\n",
      "Epoch: 04 | Time: 0m 31s\n",
      "\tTrain Loss: 3.327 | Train PPL:  27.867\n",
      "\t Val. Loss: 4.076 |  Val. PPL:  58.906\n",
      "Epoch: 05 | Time: 0m 30s\n",
      "\tTrain Loss: 3.124 | Train PPL:  22.729\n",
      "\t Val. Loss: 3.854 |  Val. PPL:  47.191\n",
      "Epoch: 06 | Time: 0m 31s\n",
      "\tTrain Loss: 2.972 | Train PPL:  19.537\n",
      "\t Val. Loss: 3.764 |  Val. PPL:  43.124\n",
      "Epoch: 07 | Time: 0m 31s\n",
      "\tTrain Loss: 2.843 | Train PPL:  17.172\n",
      "\t Val. Loss: 3.759 |  Val. PPL:  42.924\n",
      "Epoch: 08 | Time: 0m 31s\n",
      "\tTrain Loss: 2.710 | Train PPL:  15.030\n",
      "\t Val. Loss: 3.757 |  Val. PPL:  42.835\n",
      "Epoch: 09 | Time: 0m 31s\n",
      "\tTrain Loss: 2.582 | Train PPL:  13.219\n",
      "\t Val. Loss: 3.689 |  Val. PPL:  40.015\n",
      "Epoch: 10 | Time: 0m 31s\n",
      "\tTrain Loss: 2.498 | Train PPL:  12.157\n",
      "\t Val. Loss: 3.632 |  Val. PPL:  37.790\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCN Incorporated\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, ninp, nembed, nhid, nlayers, dropout):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.embedding = nn.Embedding(ninp, nembed)\n",
    "        assert(nlayers > 0)\n",
    "        layers = [GCNLayer(nembed, nhid)] + [GCNLayer(nhid, nhid) for _ in range(nlayers-1)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.linear = nn.Linear(2*nhid, nhid)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, A):\n",
    "        \"\"\"\n",
    "        x: (seq len, b)\n",
    "        A: (b, seq len, seq len)\n",
    "        \"\"\"\n",
    "        x = x.t()\n",
    "        b = x.shape[0]\n",
    "        x = self.embedding(x)  # x: (b, seq len, ninp)\n",
    "        x = self.dropout(x)\n",
    "        hidden = []\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, A) \n",
    "            hidden.append(x[:,0,:])\n",
    "            \n",
    "        # pooling\n",
    "        mean = x.mean(dim=1)\n",
    "        maxm = x.max(dim=1)[0]\n",
    "        x = torch.cat((mean, maxm), dim=1)\n",
    "        out = self.linear(self.dropout(x))\n",
    "        hidden = torch.stack(hidden)\n",
    "        return out, hidden\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, grh, tgt, teacher_forcing_ratio=0.5):\n",
    "        # src: (src_len, b)\n",
    "        # tgt: (tgt_len, b)\n",
    "        tgt_len, b = tgt.shape\n",
    "        tgt_vocab_size = self.decoder.nout\n",
    "        \n",
    "        # tensor to store decoder outputs\n",
    "        outs = torch.zeros(tgt_len, b, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        enc_out, hidden = self.encoder(src, grh)\n",
    "        x = tgt[0]\n",
    "        for t in range(1, tgt_len):\n",
    "            out, hidden = self.decoder(x, hidden)\n",
    "            outs[t] = out\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = out.argmax(1)\n",
    "            x = tgt[t] if teacher_force else top1\n",
    "            \n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n",
      "Unique tokens in source (de) vocabulary: 7855\n",
      "Unique tokens in target (en) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "REVERSE = False\n",
    "SRC = Field(tokenize = lambda text: tokenize_de(text, REVERSE), \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "TGT = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "GRH = RawField(postprocessing=batch_graph)\n",
    "data_fields = [('src', SRC), ('trg', TGT), ('grh', GRH)]\n",
    "\n",
    "train_data = Dataset(torch.load(\"data/Multi30k/train_data.pt\"), data_fields)\n",
    "valid_data = Dataset(torch.load(\"data/Multi30k/valid_data.pt\"), data_fields)\n",
    "test_data = Dataset(torch.load(\"data/Multi30k/test_data.pt\"), data_fields)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    sort_key = lambda x: len(x.src),\n",
    "    sort_within_batch=False,\n",
    "    device = device)\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TGT.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TGT.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TGT.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 700\n",
    "DEC_HID_DIM = 700\n",
    "N_LAYERS = 3\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = GCNEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = GRUDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 17,692,681 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, batch in tqdm(enumerate(iterator)):\n",
    "        src = batch.src.to(device)\n",
    "        tgt = batch.trg.to(device)\n",
    "        grh = batch.grh.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(src, grh, tgt)\n",
    "        out_dim = out.shape[-1]\n",
    "        out = out[1:].view(-1, out_dim)\n",
    "        tgt = tgt[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(out, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss/len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm(enumerate(iterator)):\n",
    "            src = batch.src.to(device)\n",
    "            tgt = batch.trg.to(device)\n",
    "            grh = batch.grh.to(device)\n",
    "            out = model(src, grh, tgt, 0) # no teacher forcing here\n",
    "            out_dim = out.shape[-1]\n",
    "            out = out[1:].view(-1, out_dim)\n",
    "            tgt = tgt[1:].view(-1)\n",
    "            loss += criterion(out, tgt).item()\n",
    "    return loss/len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start, end):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:35,  6.47it/s]\n",
      "8it [00:00, 23.45it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 35s\n",
      "\tTrain Loss: 4.834 | Train PPL: 125.671\n",
      "\t Val. Loss: 4.738 |  Val. PPL: 114.180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:35,  6.37it/s]\n",
      "8it [00:00, 22.95it/s]\n",
      "1it [00:00,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0m 35s\n",
      "\tTrain Loss: 4.186 | Train PPL:  65.776\n",
      "\t Val. Loss: 4.625 |  Val. PPL: 102.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:35,  6.44it/s]\n",
      "8it [00:00, 23.33it/s]\n",
      "1it [00:00,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0m 35s\n",
      "\tTrain Loss: 3.997 | Train PPL:  54.414\n",
      "\t Val. Loss: 4.546 |  Val. PPL:  94.277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:35,  6.36it/s]\n",
      "8it [00:00, 23.74it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0m 36s\n",
      "\tTrain Loss: 3.837 | Train PPL:  46.393\n",
      "\t Val. Loss: 4.492 |  Val. PPL:  89.339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:35,  6.38it/s]\n",
      "8it [00:00, 23.53it/s]\n",
      "1it [00:00,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0m 35s\n",
      "\tTrain Loss: 3.755 | Train PPL:  42.723\n",
      "\t Val. Loss: 4.461 |  Val. PPL:  86.607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:35,  6.43it/s]\n",
      "8it [00:00, 23.28it/s]\n",
      "1it [00:00,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Time: 0m 35s\n",
      "\tTrain Loss: 3.647 | Train PPL:  38.372\n",
      "\t Val. Loss: 4.448 |  Val. PPL:  85.462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:35,  6.40it/s]\n",
      "8it [00:00, 23.63it/s]\n",
      "1it [00:00,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Time: 0m 35s\n",
      "\tTrain Loss: 3.577 | Train PPL:  35.768\n",
      "\t Val. Loss: 4.449 |  Val. PPL:  85.520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:35,  6.37it/s]\n",
      "8it [00:00, 23.66it/s]\n",
      "1it [00:00,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Time: 0m 35s\n",
      "\tTrain Loss: 3.494 | Train PPL:  32.931\n",
      "\t Val. Loss: 4.416 |  Val. PPL:  82.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:35,  6.40it/s]\n",
      "8it [00:00, 23.11it/s]\n",
      "1it [00:00,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Time: 0m 35s\n",
      "\tTrain Loss: 3.460 | Train PPL:  31.820\n",
      "\t Val. Loss: 4.460 |  Val. PPL:  86.466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:35,  6.43it/s]\n",
      "8it [00:00, 23.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Time: 0m 35s\n",
      "\tTrain Loss: 3.379 | Train PPL:  29.339\n",
      "\t Val. Loss: 4.418 |  Val. PPL:  82.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "TGT_PAD_IDX = TGT.vocab.stoi[TGT.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TGT_PAD_IDX)\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCN and GRU Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNGRUEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, ninp, nembed, enc_nhid, dec_nhid, nlayers, dropout, device):\n",
    "        super(GCNGRUEncoder, self).__init__()\n",
    "        self.enc_nhid = enc_nhid\n",
    "        self.dec_nhid = dec_nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.embedding = nn.Embedding(ninp, nembed)\n",
    "        self.device = device\n",
    "        rnns = [nn.GRU(nembed, enc_nhid, 1, bidirectional=True)] + \\\n",
    "               [nn.GRU(enc_nhid*2, enc_nhid, 1, bidirectional=True) \n",
    "                for _ in range(nlayers-1)]\n",
    "        self.rnns = nn.ModuleList(rnns)\n",
    "        layers = [GCNLayer(nembed, enc_nhid)] + [GCNLayer(enc_nhid, enc_nhid) \n",
    "                                                   for _ in range(nlayers-1)]\n",
    "        self.gcns = nn.ModuleList(layers)\n",
    "        self.proj = nn.Linear(enc_nhid*2, enc_nhid)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, A):\n",
    "        \"\"\"\n",
    "        x: (seq len, b)\n",
    "        A: (b, seq len, seq len)\n",
    "        \"\"\"\n",
    "        s, b = x.shape\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        hiddens = []\n",
    "        hidden = torch.zeros(2, b, self.enc_nhid).to(self.device)\n",
    "        gcn_out, gru_out = embedded.transpose(1,0), embedded\n",
    "        for i in range(self.nlayers):\n",
    "            gcn_out = self.gcns[i](gcn_out, A)\n",
    "            gru_out, hidden = self.rnns[i](gru_out, hidden)\n",
    "            gru_out = self.dropout(gru_out)            \n",
    "            gru_out = gcn_out.repeat(1,1,2).transpose(0,1) + gru_out\n",
    "            gcn_out = self.proj(gru_out.transpose(0,1))\n",
    "            hidden += gcn_out.max(1)[0].repeat(2,1,1)\n",
    "            hiddens.append(hidden.sum(dim=0))\n",
    "        hidden = torch.stack(hiddens)\n",
    "        return gru_out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TGT.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "N_LAYERS = 3\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "enc = GCNGRUEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, N_LAYERS, ENC_DROPOUT, device)\n",
    "dec = GRUDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n",
      "Unique tokens in source (de) vocabulary: 7855\n",
      "Unique tokens in target (en) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "REVERSE = False\n",
    "SRC = Field(tokenize = lambda text: tokenize_de(text, REVERSE), \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "TGT = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "GRH = RawField(postprocessing=batch_graph)\n",
    "data_fields = [('src', SRC), ('trg', TGT), ('grh', GRH)]\n",
    "\n",
    "train_data = Dataset(torch.load(\"data/Multi30k/train_data.pt\"), data_fields)\n",
    "valid_data = Dataset(torch.load(\"data/Multi30k/valid_data.pt\"), data_fields)\n",
    "test_data = Dataset(torch.load(\"data/Multi30k/test_data.pt\"), data_fields)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    sort_key = lambda x: len(x.src),\n",
    "    sort_within_batch=False,\n",
    "    device = device)\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TGT.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TGT.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 23,873,797 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:43,  5.28it/s]\n",
      "8it [00:00, 23.54it/s]\n",
      "1it [00:00,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 43s\n",
      "\tTrain Loss: 4.641 | Train PPL: 103.656\n",
      "\t Val. Loss: 4.749 |  Val. PPL: 115.418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:43,  5.27it/s]\n",
      "8it [00:00, 23.14it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0m 43s\n",
      "\tTrain Loss: 3.692 | Train PPL:  40.125\n",
      "\t Val. Loss: 4.222 |  Val. PPL:  68.146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:42,  5.30it/s]\n",
      "8it [00:00, 23.81it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0m 43s\n",
      "\tTrain Loss: 3.314 | Train PPL:  27.495\n",
      "\t Val. Loss: 3.998 |  Val. PPL:  54.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:42,  5.28it/s]\n",
      "8it [00:00, 23.28it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0m 43s\n",
      "\tTrain Loss: 3.057 | Train PPL:  21.270\n",
      "\t Val. Loss: 3.901 |  Val. PPL:  49.466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:42,  5.29it/s]\n",
      "8it [00:00, 23.62it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0m 43s\n",
      "\tTrain Loss: 2.859 | Train PPL:  17.439\n",
      "\t Val. Loss: 3.871 |  Val. PPL:  47.969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:42,  5.29it/s]\n",
      "8it [00:00, 23.80it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Time: 0m 43s\n",
      "\tTrain Loss: 2.711 | Train PPL:  15.050\n",
      "\t Val. Loss: 3.978 |  Val. PPL:  53.408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:42,  5.30it/s]\n",
      "8it [00:00, 23.67it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Time: 0m 43s\n",
      "\tTrain Loss: 2.573 | Train PPL:  13.105\n",
      "\t Val. Loss: 3.854 |  Val. PPL:  47.192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:43,  5.28it/s]\n",
      "8it [00:00, 23.63it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Time: 0m 43s\n",
      "\tTrain Loss: 2.460 | Train PPL:  11.703\n",
      "\t Val. Loss: 3.926 |  Val. PPL:  50.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:42,  5.29it/s]\n",
      "8it [00:00, 23.61it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Time: 0m 43s\n",
      "\tTrain Loss: 2.375 | Train PPL:  10.753\n",
      "\t Val. Loss: 3.843 |  Val. PPL:  46.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:43,  5.27it/s]\n",
      "8it [00:00, 23.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Time: 0m 43s\n",
      "\tTrain Loss: 2.295 | Train PPL:   9.921\n",
      "\t Val. Loss: 3.862 |  Val. PPL:  47.574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "TGT_PAD_IDX = TGT.vocab.stoi[TGT.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TGT_PAD_IDX)\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCN Attention Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.gru_attn import Attention, GRUDecoder\n",
    "\n",
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, ninp, nembed, enc_nhid, dec_nhid, nlayers, dropout):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.enc_nhid = enc_nhid\n",
    "        self.dec_nhid = dec_nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.embedding = nn.Embedding(ninp, nembed)\n",
    "        assert(nlayers > 0)\n",
    "        layers = [GCNLayer(nembed, enc_nhid)] + \\\n",
    "                 [GCNLayer(enc_nhid, enc_nhid) for _ in range(nlayers-1)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.out_linear = nn.Linear(enc_nhid, 2*enc_nhid)\n",
    "        self.hid_linear = nn.Linear(enc_nhid, dec_nhid)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, A):\n",
    "        \"\"\"\n",
    "        x: (seq len, b)\n",
    "        A: (b, seq len, seq len)\n",
    "        \"\"\"\n",
    "        x = x.t()\n",
    "        b = x.shape[0]\n",
    "        x = self.embedding(x)  # x: (b, seq len, ninp)\n",
    "        x = self.dropout(x)\n",
    "        hidden = []\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, A) \n",
    "            hidden.append(x[:,0,:])\n",
    "            \n",
    "        # pooling\n",
    "        out = self.out_linear(self.dropout(x)).transpose(1,0)\n",
    "        hidden = torch.stack(hidden)\n",
    "        hidden = self.hid_linear(hidden)\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "class GCNGRUEncoder(nn.Module):\n",
    "    def __init__(self, ninp, nembed, enc_nhid, dec_nhid, nlayers, dropout, device=torch.device('cpu')):\n",
    "        super(GCNGRUEncoder, self).__init__()\n",
    "        self.enc_nhid = enc_nhid\n",
    "        self.dec_nhid = dec_nhid\n",
    "        self.nlayers = nlayers\n",
    "        assert(nlayers > 0)\n",
    "        self.embedding = nn.Embedding(ninp, nembed)\n",
    "        grus = [nn.GRU(nembed, enc_nhid, 1, bidirectional=True)] + \\\n",
    "               [nn.GRU(enc_nhid*2, enc_nhid, 1, bidirectional=True)\n",
    "                for _ in range(nlayers-1)] \n",
    "        gcns = [GCNLayer(nembed, enc_nhid)] + \\\n",
    "               [GCNLayer(enc_nhid, enc_nhid) for _ in range(nlayers-1)]\n",
    "        self.grus = nn.ModuleList(grus)\n",
    "        self.gcns = nn.ModuleList(gcns)\n",
    "        self.proj = nn.Linear(enc_nhid*2, enc_nhid)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, x, A):\n",
    "        s, b = x.shape\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        hiddens = []\n",
    "        hidden = torch.zeros(2, b, self.enc_nhid).to(self.device)\n",
    "        gcn_out, gru_out = embedded.transpose(1,0), embedded\n",
    "        for i in range(self.nlayers):\n",
    "            gcn_out = self.gcns[i](gcn_out, A)\n",
    "            gru_out, hidden = self.grus[i](gru_out, hidden)\n",
    "            gru_out = self.dropout(gru_out)\n",
    "            gru_out = gcn_out.repeat(1,1,2).transpose(0,1) + gru_out\n",
    "            gcn_out = self.proj(gru_out.transpose(0,1))\n",
    "            hidden += gcn_out.max(1)[0].repeat(2,1,1)\n",
    "            hiddens.append(hidden.sum(dim=0))\n",
    "        hidden = torch.stack(hiddens)\n",
    "        return gru_out, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(GCN2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, grh, tgt, teacher_forcing_ratio=0.5):\n",
    "        # src: (src_len, b)\n",
    "        # tgt: (tgt_len, b)\n",
    "        tgt_len, b = tgt.shape\n",
    "        tgt_vocab_size = self.decoder.nout\n",
    "        \n",
    "        # tensor to store decoder outputs\n",
    "        outs = torch.zeros(tgt_len, b, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        enc_out, hidden = self.encoder(src, grh)\n",
    "        x = tgt[0]\n",
    "        attns = []\n",
    "        for t in range(1, tgt_len):\n",
    "            out, hidden, attn = self.decoder(x, hidden, enc_out)\n",
    "            attns.append(attn)\n",
    "            outs[t] = out\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = out.argmax(1)\n",
    "            x = tgt[t] if teacher_force else top1\n",
    "        attns = torch.stack(attns, dim=1).squeeze()\n",
    "        return outs, attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n",
      "Unique tokens in source (de) vocabulary: 7855\n",
      "Unique tokens in target (en) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "REVERSE = False\n",
    "SRC = Field(tokenize = lambda text: tokenize_de(text, REVERSE), \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "TGT = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "GRH = RawField(postprocessing=batch_graph)\n",
    "data_fields = [('src', SRC), ('trg', TGT), ('grh', GRH)]\n",
    "\n",
    "train_data = Dataset(torch.load(\"data/Multi30k/train_data.pt\"), data_fields)\n",
    "valid_data = Dataset(torch.load(\"data/Multi30k/valid_data.pt\"), data_fields)\n",
    "test_data = Dataset(torch.load(\"data/Multi30k/test_data.pt\"), data_fields)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    sort_key = lambda x: len(x.src),\n",
    "    sort_within_batch=False,\n",
    "    device = device)\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TGT.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TGT.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TGT.vocab)\n",
    "ENC_EMB_DIM = 250\n",
    "DEC_EMB_DIM = 250\n",
    "ENC_HID_DIM = 500\n",
    "DEC_HID_DIM = 500\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "NLAYERS = 2\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "# enc = GCNEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, NLAYERS, ENC_DROPOUT)\n",
    "enc = GCNGRUEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, NLAYERS, ENC_DROPOUT, device)\n",
    "dec = GRUDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, NLAYERS, DEC_DROPOUT, attn)\n",
    "model = GCN2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 26,276,143 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, batch in tqdm(enumerate(iterator)):\n",
    "        src = batch.src.to(device)\n",
    "        tgt = batch.trg.to(device)\n",
    "        grh = batch.grh.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out, attns = model(src, grh, tgt)\n",
    "        out_dim = out.shape[-1]\n",
    "        out = out[1:].view(-1, out_dim)\n",
    "        tgt = tgt[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(out, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss/len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm(enumerate(iterator)):\n",
    "            src = batch.src.to(device)\n",
    "            tgt = batch.trg.to(device)\n",
    "            grh = batch.grh.to(device)\n",
    "            out, attns = model(src, grh, tgt, 0) # no teacher forcing here\n",
    "            out_dim = out.shape[-1]\n",
    "            out = out[1:].view(-1, out_dim)\n",
    "            tgt = tgt[1:].view(-1)\n",
    "            loss += criterion(out, tgt).item()\n",
    "    return loss/len(iterator)\n",
    "\n",
    "def epoch_time(start, end):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-3e9eafece2d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-7eed5dd10f7a>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "TGT_PAD_IDX = TGT.vocab.stoi[TGT.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TGT_PAD_IDX)\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCNGRU Attention Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baby test for GCN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GCNGRUEncoder' object has no attribute 'embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-90ae189cddd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-65d053eccb23>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, A)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mhiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_nhid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    533\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 535\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GCNGRUEncoder' object has no attribute 'embedding'"
     ]
    }
   ],
   "source": [
    "for b in train_iterator:\n",
    "    x = b.src.to(device)\n",
    "    A = b.grh.to(device)\n",
    "    break\n",
    "\n",
    "out, hidden = enc(x, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hidden' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a83816c89964>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'hidden' is not defined"
     ]
    }
   ],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
